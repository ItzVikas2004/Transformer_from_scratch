{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12375108,"sourceType":"datasetVersion","datasetId":7802936}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:11.163005Z","iopub.execute_input":"2025-07-04T18:45:11.163377Z","iopub.status.idle":"2025-07-04T18:45:11.174680Z","shell.execute_reply.started":"2025-07-04T18:45:11.163348Z","shell.execute_reply":"2025-07-04T18:45:11.173702Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/input-1-1/input (1).txt\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:14.471468Z","iopub.execute_input":"2025-07-04T18:45:14.471798Z","iopub.status.idle":"2025-07-04T18:45:14.476256Z","shell.execute_reply.started":"2025-07-04T18:45:14.471775Z","shell.execute_reply":"2025-07-04T18:45:14.475550Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"BATCH_SIZE = 16       # Number of sequences processed in parallel\nBLOCK_SIZE = 32       # Maximum context length for predictions\nMAX_ITERS = 5000      # Training iterations\nEVAL_INTERVAL = 100   # Interval to evaluate loss\nLEARNING_RATE = 1e-3  # Learning rate for optimizer\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nEVAL_ITERS = 200      # Iterations to estimate loss\nEMBED_DIM = 64        # Embedding dimension\nNUM_HEADS = 4         # Number of attention heads\nNUM_LAYERS = 4        # Number of Transformer blocks\nDROPOUT = 0.0         # Dropout probability\n\ntorch.manual_seed(1337)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:17.845692Z","iopub.execute_input":"2025-07-04T18:45:17.846002Z","iopub.status.idle":"2025-07-04T18:45:17.855379Z","shell.execute_reply.started":"2025-07-04T18:45:17.845980Z","shell.execute_reply":"2025-07-04T18:45:17.854449Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7b4336777090>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"with open('/kaggle/input/input-1-1/input (1).txt', 'r', encoding='utf-8') as f:\n    raw_text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:21.010392Z","iopub.execute_input":"2025-07-04T18:45:21.010775Z","iopub.status.idle":"2025-07-04T18:45:21.041065Z","shell.execute_reply.started":"2025-07-04T18:45:21.010749Z","shell.execute_reply":"2025-07-04T18:45:21.040142Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Vocabulary and encoders\nvocab = sorted(list(set(raw_text)))\nvocab_size = len(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for i, ch in enumerate(vocab)}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda indices: ''.join([itos[i] for i in indices])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:24.870275Z","iopub.execute_input":"2025-07-04T18:45:24.870595Z","iopub.status.idle":"2025-07-04T18:45:24.890806Z","shell.execute_reply.started":"2025-07-04T18:45:24.870570Z","shell.execute_reply":"2025-07-04T18:45:24.889761Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(stoi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:53:02.254784Z","iopub.execute_input":"2025-07-04T18:53:02.255030Z","iopub.status.idle":"2025-07-04T18:53:02.274361Z","shell.execute_reply.started":"2025-07-04T18:53:02.255008Z","shell.execute_reply":"2025-07-04T18:53:02.273435Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Encode data and split\ndata = torch.tensor(encode(raw_text), dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:27.700433Z","iopub.execute_input":"2025-07-04T18:45:27.701294Z","iopub.status.idle":"2025-07-04T18:45:27.923786Z","shell.execute_reply.started":"2025-07-04T18:45:27.701251Z","shell.execute_reply":"2025-07-04T18:45:27.922843Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_batch(split):\n    \"\"\"Generate a batch of input & target sequences.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n    return x.to(DEVICE), y.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:30.953013Z","iopub.execute_input":"2025-07-04T18:45:30.953306Z","iopub.status.idle":"2025-07-04T18:45:30.959640Z","shell.execute_reply.started":"2025-07-04T18:45:30.953282Z","shell.execute_reply":"2025-07-04T18:45:30.958574Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    \"\"\"Estimate train and validation loss.\"\"\"\n    results = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(EVAL_ITERS)\n        for k in range(EVAL_ITERS):\n            xb, yb = get_batch(split)\n            _, loss = model(xb, yb)\n            losses[k] = loss.item()\n        results[split] = losses.mean()\n    model.train()\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:34.144950Z","iopub.execute_input":"2025-07-04T18:45:34.145250Z","iopub.status.idle":"2025-07-04T18:45:34.151150Z","shell.execute_reply.started":"2025-07-04T18:45:34.145219Z","shell.execute_reply":"2025-07-04T18:45:34.150172Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class SelfAttentionHead(nn.Module):\n    \"\"\"Single masked self-attention head.\"\"\"\n\n    def __init__(self, head_dim):\n        super().__init__()\n        self.key = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.query = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.value = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        attn_weights = q @ k.transpose(-2, -1) * C ** -0.5\n        attn_weights = attn_weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        v = self.value(x)\n        out = attn_weights @ v\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:37.488393Z","iopub.execute_input":"2025-07-04T18:45:37.488794Z","iopub.status.idle":"2025-07-04T18:45:37.496112Z","shell.execute_reply.started":"2025-07-04T18:45:37.488770Z","shell.execute_reply":"2025-07-04T18:45:37.495338Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    \"\"\"Parallel multiple self-attention heads.\"\"\"\n\n    def __init__(self, num_heads, head_dim):\n        super().__init__()\n        self.heads = nn.ModuleList([SelfAttentionHead(head_dim) for _ in range(num_heads)])\n        self.proj = nn.Linear(EMBED_DIM, EMBED_DIM)\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:41.003385Z","iopub.execute_input":"2025-07-04T18:45:41.003725Z","iopub.status.idle":"2025-07-04T18:45:41.009824Z","shell.execute_reply.started":"2025-07-04T18:45:41.003702Z","shell.execute_reply":"2025-07-04T18:45:41.008835Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    \"\"\"Position-wise feed-forward network.\"\"\"\n\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n            nn.Dropout(DROPOUT),\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:44.862000Z","iopub.execute_input":"2025-07-04T18:45:44.862355Z","iopub.status.idle":"2025-07-04T18:45:44.867795Z","shell.execute_reply.started":"2025-07-04T18:45:44.862328Z","shell.execute_reply":"2025-07-04T18:45:44.866705Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"Single Transformer block.\"\"\"\n\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        head_dim = embed_dim // num_heads\n        self.self_attn = MultiHeadSelfAttention(num_heads, head_dim)\n        self.ffn = FeedForwardNetwork(embed_dim)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = x + self.self_attn(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:48.522008Z","iopub.execute_input":"2025-07-04T18:45:48.522316Z","iopub.status.idle":"2025-07-04T18:45:48.528062Z","shell.execute_reply.started":"2025-07-04T18:45:48.522293Z","shell.execute_reply":"2025-07-04T18:45:48.527178Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class TransformerLanguageModel(nn.Module):\n    \"\"\"Character-level Transformer language model.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, EMBED_DIM)\n        self.position_embedding = nn.Embedding(BLOCK_SIZE, EMBED_DIM)\n        self.transformer_blocks = nn.Sequential(*[TransformerBlock(EMBED_DIM, NUM_HEADS) for _ in range(NUM_LAYERS)])\n        self.ln_f = nn.LayerNorm(EMBED_DIM)\n        self.output_head = nn.Linear(EMBED_DIM, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding(idx)\n        pos_emb = self.position_embedding(torch.arange(T, device=DEVICE))\n        x = tok_emb + pos_emb\n        x = self.transformer_blocks(x)\n        x = self.ln_f(x)\n        logits = self.output_head(x)\n\n        loss = None\n        if targets is not None:\n            logits = logits.view(B * T, vocab_size)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new tokens autoregressively.\"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -BLOCK_SIZE:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, next_token], dim=1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:52.626535Z","iopub.execute_input":"2025-07-04T18:45:52.626854Z","iopub.status.idle":"2025-07-04T18:45:52.636888Z","shell.execute_reply.started":"2025-07-04T18:45:52.626831Z","shell.execute_reply":"2025-07-04T18:45:52.635887Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model = TransformerLanguageModel().to(DEVICE)\nprint(f\"Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\nfor step in range(MAX_ITERS):\n\n    if step % EVAL_INTERVAL == 0 or step == MAX_ITERS - 1:\n        losses = estimate_loss()\n        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:45:55.809875Z","iopub.execute_input":"2025-07-04T18:45:55.810171Z","iopub.status.idle":"2025-07-04T18:53:02.245359Z","shell.execute_reply.started":"2025-07-04T18:45:55.810149Z","shell.execute_reply":"2025-07-04T18:53:02.244360Z"}},"outputs":[{"name":"stdout","text":"Model has 0.21M parameters\nStep 0: train loss 3.6067, val loss 3.6052\nStep 100: train loss 1.6236, val loss 1.6175\nStep 200: train loss 0.6831, val loss 0.6801\nStep 300: train loss 0.2766, val loss 0.2769\nStep 400: train loss 0.2184, val loss 0.2156\nStep 500: train loss 0.1924, val loss 0.1942\nStep 600: train loss 0.1928, val loss 0.1924\nStep 700: train loss 0.1890, val loss 0.1874\nStep 800: train loss 0.1830, val loss 0.1799\nStep 900: train loss 0.1835, val loss 0.1807\nStep 1000: train loss 0.1785, val loss 0.1774\nStep 1100: train loss 0.1825, val loss 0.1863\nStep 1200: train loss 0.1737, val loss 0.1717\nStep 1300: train loss 0.1792, val loss 0.1791\nStep 1400: train loss 0.1749, val loss 0.1733\nStep 1500: train loss 0.1732, val loss 0.1727\nStep 1600: train loss 0.1712, val loss 0.1734\nStep 1700: train loss 0.1752, val loss 0.1729\nStep 1800: train loss 0.1803, val loss 0.1811\nStep 1900: train loss 0.1712, val loss 0.1682\nStep 2000: train loss 0.1708, val loss 0.1719\nStep 2100: train loss 0.1729, val loss 0.1696\nStep 2200: train loss 0.1673, val loss 0.1666\nStep 2300: train loss 0.1813, val loss 0.1821\nStep 2400: train loss 0.1731, val loss 0.1744\nStep 2500: train loss 0.1713, val loss 0.1701\nStep 2600: train loss 0.1650, val loss 0.1644\nStep 2700: train loss 0.1718, val loss 0.1714\nStep 2800: train loss 0.1675, val loss 0.1676\nStep 2900: train loss 0.1688, val loss 0.1699\nStep 3000: train loss 0.1634, val loss 0.1643\nStep 3100: train loss 0.1729, val loss 0.1707\nStep 3200: train loss 0.1673, val loss 0.1679\nStep 3300: train loss 0.1665, val loss 0.1673\nStep 3400: train loss 0.1685, val loss 0.1678\nStep 3500: train loss 0.1665, val loss 0.1665\nStep 3600: train loss 0.1670, val loss 0.1663\nStep 3700: train loss 0.1676, val loss 0.1655\nStep 3800: train loss 0.1647, val loss 0.1653\nStep 3900: train loss 0.1644, val loss 0.1648\nStep 4000: train loss 0.1678, val loss 0.1656\nStep 4100: train loss 0.1713, val loss 0.1704\nStep 4200: train loss 0.1689, val loss 0.1683\nStep 4300: train loss 0.1683, val loss 0.1673\nStep 4400: train loss 0.1664, val loss 0.1662\nStep 4500: train loss 0.1643, val loss 0.1655\nStep 4600: train loss 0.1671, val loss 0.1659\nStep 4700: train loss 0.1649, val loss 0.1650\nStep 4800: train loss 0.1643, val loss 0.1631\nStep 4900: train loss 0.1645, val loss 0.1635\nStep 4999: train loss 0.1641, val loss 0.1655\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\ngenerated_indices = model.generate(context, max_new_tokens=500)\nprint(decode(generated_indices[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T19:01:26.147099Z","iopub.execute_input":"2025-07-04T19:01:26.147439Z","iopub.status.idle":"2025-07-04T19:01:29.852610Z","shell.execute_reply.started":"2025-07-04T19:01:26.147413Z","shell.execute_reply":"2025-07-04T19:01:29.851880Z"}},"outputs":[{"name":"stdout","text":"\n\n\nThe rain falls softly on the ground. A gentle breeze moves the leaves. The stars twinkle in the clear night sky. The sun shines bright in the sky. The cat sat on the mat. The tree stands tall and strong. The cat sat on the mat. The sun shines bright in the sky. The dog barked loudly at night. The dog barked loudly at night. The sun shines bright in the sky. The tree stands tall and strong. The dog barked loudly at night. Children play happily in the park. The cat sat on the mat. The cat sat on\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\n\n\ncontext = \"i am going to marke\"\n\n\ncontext_ids = torch.tensor([encode(context)], dtype=torch.long)\nprint(f\"Encoded context: {context_ids}\")\n\n\noutput = model.generate(idx=context_ids, max_new_tokens=1)\n\nnext_char_id = output[0][-1].item()\nnext_char = decode([next_char_id])\n\nprint(f\"Given '{context}' => Next character predicted: '{next_char}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T19:04:41.450064Z","iopub.execute_input":"2025-07-04T19:04:41.450396Z","iopub.status.idle":"2025-07-04T19:04:41.467738Z","shell.execute_reply.started":"2025-07-04T19:04:41.450371Z","shell.execute_reply":"2025-07-04T19:04:41.466770Z"}},"outputs":[{"name":"stdout","text":"Encoded context: tensor([[15,  1,  7, 18,  1, 13, 20, 15, 19, 13,  1, 24, 20,  1, 18,  7, 22, 16,\n         11]])\nGiven 'i am going to marke' => Next character predicted: 'd'\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"@torch.no_grad()\ndef test_accuracy(split='val', num_batches=100):\n    model.eval()\n    total = 0\n    correct = 0\n\n    for _ in range(num_batches):\n        xb, yb = get_batch(split)  # [B, T]\n        logits, _ = model(xb, yb)  # logits: [B*T, vocab_size]\n\n        # logits shape: [B*T, vocab_size]\n        preds = torch.argmax(logits, dim=-1)  # [B*T]\n\n        B, T = xb.shape\n        preds = preds.view(B, T)  # ✅ Unflatten to [B, T]\n\n        assert preds.shape == yb.shape, f\"Shape mismatch: {preds.shape} vs {yb.shape}\"\n\n        matches = preds == yb  # [B, T]\n        correct += matches.sum().item()\n        total += matches.numel()\n\n    model.train()\n    accuracy = correct / total * 100\n    print(f\"{split.capitalize()} accuracy: {accuracy:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T19:04:49.002899Z","iopub.execute_input":"2025-07-04T19:04:49.003209Z","iopub.status.idle":"2025-07-04T19:04:49.010855Z","shell.execute_reply.started":"2025-07-04T19:04:49.003178Z","shell.execute_reply":"2025-07-04T19:04:49.009697Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"test_accuracy(split='val', num_batches=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T19:04:52.429395Z","iopub.execute_input":"2025-07-04T19:04:52.429746Z","iopub.status.idle":"2025-07-04T19:04:53.542598Z","shell.execute_reply.started":"2025-07-04T19:04:52.429723Z","shell.execute_reply":"2025-07-04T19:04:53.541714Z"}},"outputs":[{"name":"stdout","text":"Val accuracy: 93.59%\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"93.587890625"},"metadata":{}}],"execution_count":30}]}