{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12218353,"sourceType":"datasetVersion","datasetId":7697637}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:40.941131Z","iopub.execute_input":"2025-06-19T13:49:40.941939Z","iopub.status.idle":"2025-06-19T13:49:40.945694Z","shell.execute_reply.started":"2025-06-19T13:49:40.941911Z","shell.execute_reply":"2025-06-19T13:49:40.944838Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"BATCH_SIZE = 16       # Number of sequences processed in parallel\nBLOCK_SIZE = 32       # Maximum context length for predictions\nMAX_ITERS = 5000      # Training iterations\nEVAL_INTERVAL = 100   # Interval to evaluate loss\nLEARNING_RATE = 1e-3  # Learning rate for optimizer\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nEVAL_ITERS = 200      # Iterations to estimate loss\nEMBED_DIM = 64        # Embedding dimension\nNUM_HEADS = 4         # Number of attention heads\nNUM_LAYERS = 4        # Number of Transformer blocks\nDROPOUT = 0.0         # Dropout probability\n\ntorch.manual_seed(1337)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:43.524918Z","iopub.execute_input":"2025-06-19T13:49:43.525203Z","iopub.status.idle":"2025-06-19T13:49:43.533066Z","shell.execute_reply.started":"2025-06-19T13:49:43.525184Z","shell.execute_reply":"2025-06-19T13:49:43.532265Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c5eeff548d0>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"with open('/kaggle/input/input/input.txt', 'r', encoding='utf-8') as f:\n    raw_text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:47.867078Z","iopub.execute_input":"2025-06-19T13:49:47.867688Z","iopub.status.idle":"2025-06-19T13:49:47.872898Z","shell.execute_reply.started":"2025-06-19T13:49:47.867660Z","shell.execute_reply":"2025-06-19T13:49:47.872110Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Vocabulary and encoders\nvocab = sorted(list(set(raw_text)))\nvocab_size = len(vocab)\nstoi = {ch: i for i, ch in enumerate(vocab)}\nitos = {i: ch for i, ch in enumerate(vocab)}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda indices: ''.join([itos[i] for i in indices])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:50.544868Z","iopub.execute_input":"2025-06-19T13:49:50.545187Z","iopub.status.idle":"2025-06-19T13:49:50.562753Z","shell.execute_reply.started":"2025-06-19T13:49:50.545162Z","shell.execute_reply":"2025-06-19T13:49:50.561846Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Encode data and split\ndata = torch.tensor(encode(raw_text), dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:52.941859Z","iopub.execute_input":"2025-06-19T13:49:52.942184Z","iopub.status.idle":"2025-06-19T13:49:53.098216Z","shell.execute_reply.started":"2025-06-19T13:49:52.942158Z","shell.execute_reply":"2025-06-19T13:49:53.097229Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def get_batch(split):\n    \"\"\"Generate a batch of input & target sequences.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n    return x.to(DEVICE), y.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:55.872894Z","iopub.execute_input":"2025-06-19T13:49:55.873188Z","iopub.status.idle":"2025-06-19T13:49:55.878518Z","shell.execute_reply.started":"2025-06-19T13:49:55.873168Z","shell.execute_reply":"2025-06-19T13:49:55.877642Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    \"\"\"Estimate train and validation loss.\"\"\"\n    results = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(EVAL_ITERS)\n        for k in range(EVAL_ITERS):\n            xb, yb = get_batch(split)\n            _, loss = model(xb, yb)\n            losses[k] = loss.item()\n        results[split] = losses.mean()\n    model.train()\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:49:57.941904Z","iopub.execute_input":"2025-06-19T13:49:57.942216Z","iopub.status.idle":"2025-06-19T13:49:57.947781Z","shell.execute_reply.started":"2025-06-19T13:49:57.942194Z","shell.execute_reply":"2025-06-19T13:49:57.947015Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class SelfAttentionHead(nn.Module):\n    \"\"\"Single masked self-attention head.\"\"\"\n\n    def __init__(self, head_dim):\n        super().__init__()\n        self.key = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.query = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.value = nn.Linear(EMBED_DIM, head_dim, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        attn_weights = q @ k.transpose(-2, -1) * C ** -0.5\n        attn_weights = attn_weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        v = self.value(x)\n        out = attn_weights @ v\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:00.295063Z","iopub.execute_input":"2025-06-19T13:50:00.295395Z","iopub.status.idle":"2025-06-19T13:50:00.302836Z","shell.execute_reply.started":"2025-06-19T13:50:00.295372Z","shell.execute_reply":"2025-06-19T13:50:00.301821Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    \"\"\"Parallel multiple self-attention heads.\"\"\"\n\n    def __init__(self, num_heads, head_dim):\n        super().__init__()\n        self.heads = nn.ModuleList([SelfAttentionHead(head_dim) for _ in range(num_heads)])\n        self.proj = nn.Linear(EMBED_DIM, EMBED_DIM)\n        self.dropout = nn.Dropout(DROPOUT)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:03.127957Z","iopub.execute_input":"2025-06-19T13:50:03.128257Z","iopub.status.idle":"2025-06-19T13:50:03.134388Z","shell.execute_reply.started":"2025-06-19T13:50:03.128233Z","shell.execute_reply":"2025-06-19T13:50:03.133443Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    \"\"\"Position-wise feed-forward network.\"\"\"\n\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.ReLU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n            nn.Dropout(DROPOUT),\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:05.728199Z","iopub.execute_input":"2025-06-19T13:50:05.728971Z","iopub.status.idle":"2025-06-19T13:50:05.733895Z","shell.execute_reply.started":"2025-06-19T13:50:05.728944Z","shell.execute_reply":"2025-06-19T13:50:05.733077Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"Single Transformer block.\"\"\"\n\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        head_dim = embed_dim // num_heads\n        self.self_attn = MultiHeadSelfAttention(num_heads, head_dim)\n        self.ffn = FeedForwardNetwork(embed_dim)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = x + self.self_attn(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:07.998983Z","iopub.execute_input":"2025-06-19T13:50:07.999281Z","iopub.status.idle":"2025-06-19T13:50:08.005216Z","shell.execute_reply.started":"2025-06-19T13:50:07.999258Z","shell.execute_reply":"2025-06-19T13:50:08.004350Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class TransformerLanguageModel(nn.Module):\n    \"\"\"Character-level Transformer language model.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, EMBED_DIM)\n        self.position_embedding = nn.Embedding(BLOCK_SIZE, EMBED_DIM)\n        self.transformer_blocks = nn.Sequential(*[TransformerBlock(EMBED_DIM, NUM_HEADS) for _ in range(NUM_LAYERS)])\n        self.ln_f = nn.LayerNorm(EMBED_DIM)\n        self.output_head = nn.Linear(EMBED_DIM, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding(idx)\n        pos_emb = self.position_embedding(torch.arange(T, device=DEVICE))\n        x = tok_emb + pos_emb\n        x = self.transformer_blocks(x)\n        x = self.ln_f(x)\n        logits = self.output_head(x)\n\n        loss = None\n        if targets is not None:\n            logits = logits.view(B * T, vocab_size)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new tokens autoregressively.\"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -BLOCK_SIZE:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat([idx, next_token], dim=1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:10.316239Z","iopub.execute_input":"2025-06-19T13:50:10.316543Z","iopub.status.idle":"2025-06-19T13:50:10.327367Z","shell.execute_reply.started":"2025-06-19T13:50:10.316523Z","shell.execute_reply":"2025-06-19T13:50:10.326477Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"model = TransformerLanguageModel().to(DEVICE)\nprint(f\"Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\nfor step in range(MAX_ITERS):\n\n    if step % EVAL_INTERVAL == 0 or step == MAX_ITERS - 1:\n        losses = estimate_loss()\n        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:50:14.085002Z","iopub.execute_input":"2025-06-19T13:50:14.085347Z","iopub.status.idle":"2025-06-19T13:56:36.093259Z","shell.execute_reply.started":"2025-06-19T13:50:14.085321Z","shell.execute_reply":"2025-06-19T13:56:36.092491Z"}},"outputs":[{"name":"stdout","text":"Model has 0.21M parameters\nStep 0: train loss 4.4116, val loss 4.4022\nStep 100: train loss 2.6568, val loss 2.6670\nStep 200: train loss 2.5089, val loss 2.5057\nStep 300: train loss 2.4194, val loss 2.4336\nStep 400: train loss 2.3502, val loss 2.3565\nStep 500: train loss 2.2965, val loss 2.3131\nStep 600: train loss 2.2406, val loss 2.2497\nStep 700: train loss 2.2046, val loss 2.2187\nStep 800: train loss 2.1634, val loss 2.1868\nStep 900: train loss 2.1237, val loss 2.1503\nStep 1000: train loss 2.1028, val loss 2.1298\nStep 1100: train loss 2.0689, val loss 2.1171\nStep 1200: train loss 2.0388, val loss 2.0797\nStep 1300: train loss 2.0248, val loss 2.0635\nStep 1400: train loss 1.9918, val loss 2.0363\nStep 1500: train loss 1.9696, val loss 2.0305\nStep 1600: train loss 1.9642, val loss 2.0493\nStep 1700: train loss 1.9411, val loss 2.0145\nStep 1800: train loss 1.9079, val loss 1.9959\nStep 1900: train loss 1.9079, val loss 1.9883\nStep 2000: train loss 1.8835, val loss 1.9953\nStep 2100: train loss 1.8699, val loss 1.9753\nStep 2200: train loss 1.8574, val loss 1.9599\nStep 2300: train loss 1.8553, val loss 1.9534\nStep 2400: train loss 1.8402, val loss 1.9428\nStep 2500: train loss 1.8167, val loss 1.9444\nStep 2600: train loss 1.8290, val loss 1.9421\nStep 2700: train loss 1.8109, val loss 1.9324\nStep 2800: train loss 1.8038, val loss 1.9232\nStep 2900: train loss 1.8016, val loss 1.9273\nStep 3000: train loss 1.7964, val loss 1.9227\nStep 3100: train loss 1.7684, val loss 1.9163\nStep 3200: train loss 1.7521, val loss 1.9114\nStep 3300: train loss 1.7578, val loss 1.9079\nStep 3400: train loss 1.7553, val loss 1.8942\nStep 3500: train loss 1.7372, val loss 1.8915\nStep 3600: train loss 1.7251, val loss 1.8866\nStep 3700: train loss 1.7287, val loss 1.8801\nStep 3800: train loss 1.7182, val loss 1.8922\nStep 3900: train loss 1.7223, val loss 1.8756\nStep 4000: train loss 1.7114, val loss 1.8564\nStep 4100: train loss 1.7090, val loss 1.8750\nStep 4200: train loss 1.7048, val loss 1.8670\nStep 4300: train loss 1.7029, val loss 1.8482\nStep 4400: train loss 1.7053, val loss 1.8661\nStep 4500: train loss 1.6914, val loss 1.8536\nStep 4600: train loss 1.6842, val loss 1.8348\nStep 4700: train loss 1.6839, val loss 1.8447\nStep 4800: train loss 1.6651, val loss 1.8425\nStep 4900: train loss 1.6718, val loss 1.8368\nStep 4999: train loss 1.6628, val loss 1.8235\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\ngenerated_indices = model.generate(context, max_new_tokens=500)\nprint(decode(generated_indices[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:57:52.076064Z","iopub.execute_input":"2025-06-19T13:57:52.076991Z","iopub.status.idle":"2025-06-19T13:57:55.574279Z","shell.execute_reply.started":"2025-06-19T13:57:52.076960Z","shell.execute_reply":"2025-06-19T13:57:55.573442Z"}},"outputs":[{"name":"stdout","text":"\nRettomen gives freign:\nMy swarme Volivius: you have It Dork's by I done have but\nhis noble all.\n\nNORGEO:\nYou! but you litt not on, you glet not tile eybell'd supo handst not not\nstruth my behish dove, for thom tow, go:\nEve mirgues warges shall; there some not.\n\nLUCIO:\nhis his love.\n\nHENRY VI:\nNo man that they before.\n\nBRUTUS:\nTows; I, I sidget and alind worge the dreads togenes!\n\nCLARENCE:\nAll was maither,\nAnd lidst worsay this like to-does\nbe e'll comfsire with thou strave so grave.\n\nMENENIUS:\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import torch\n\n\ncontext = \"gi\"\n\n\ncontext_ids = torch.tensor([encode(context)], dtype=torch.long)\nprint(f\"Encoded context: {context_ids}\")\n\n\noutput = m.generate(idx=context_ids, max_new_tokens=1)\n\nnext_char_id = output[0][-1].item()\nnext_char = decode([next_char_id])\n\nprint(f\"Given '{context}' => Next character predicted: '{next_char}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:57:59.318626Z","iopub.execute_input":"2025-06-19T13:57:59.318978Z","iopub.status.idle":"2025-06-19T13:57:59.334235Z","shell.execute_reply.started":"2025-06-19T13:57:59.318950Z","shell.execute_reply":"2025-06-19T13:57:59.333470Z"}},"outputs":[{"name":"stdout","text":"Encoded context: tensor([[45, 47]])\nGiven 'gi' => Next character predicted: 'n'\n","output_type":"stream"}],"execution_count":38}]}