{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12214980,"sourceType":"datasetVersion","datasetId":7695239},{"sourceId":12217676,"sourceType":"datasetVersion","datasetId":7697157}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:18:46.976305Z","iopub.execute_input":"2025-06-19T11:18:46.976654Z","iopub.status.idle":"2025-06-19T11:18:46.994505Z","shell.execute_reply.started":"2025-06-19T11:18:46.976626Z","shell.execute_reply":"2025-06-19T11:18:46.993412Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/input-1/input.txt\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:05.265977Z","iopub.execute_input":"2025-06-19T12:10:05.266348Z","iopub.status.idle":"2025-06-19T12:10:05.294142Z","shell.execute_reply.started":"2025-06-19T12:10:05.266322Z","shell.execute_reply":"2025-06-19T12:10:05.293041Z"}},"outputs":[{"name":"stdout","text":"\n .ABCTabcdefghiklmnoprstuvwyz\n30\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:08.094981Z","iopub.execute_input":"2025-06-19T12:10:08.095278Z","iopub.status.idle":"2025-06-19T12:10:08.102110Z","shell.execute_reply.started":"2025-06-19T12:10:08.095255Z","shell.execute_reply":"2025-06-19T12:10:08.101016Z"}},"outputs":[{"name":"stdout","text":"[14, 15, 15, 1, 24, 14, 11, 22, 11]\nhii there\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"import torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:10.679979Z","iopub.execute_input":"2025-06-19T12:10:10.680313Z","iopub.status.idle":"2025-06-19T12:10:10.870411Z","shell.execute_reply.started":"2025-06-19T12:10:10.680288Z","shell.execute_reply":"2025-06-19T12:10:10.869475Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1357143]) torch.int64\ntensor([ 6, 14, 11,  1, 23, 25, 19,  1, 23, 14, 15, 19, 11, 23,  1,  8, 22, 15,\n        13, 14, 24,  1, 15, 19,  1, 24, 14, 11,  1, 23, 16, 28,  2,  1,  6, 14,\n        11,  1, 23, 25, 19,  1, 23, 14, 15, 19, 11, 23,  1,  8, 22, 15, 13, 14,\n        24,  1, 15, 19,  1, 24, 14, 11,  1, 23, 16, 28,  2,  1,  5, 14, 15, 17,\n        10, 22, 11, 19,  1, 21, 17,  7, 28,  1, 14,  7, 21, 21, 15, 17, 28,  1,\n        15, 19,  1, 24, 14, 11,  1, 21,  7, 22, 16,  2,  1,  6, 14, 11,  1, 10,\n        20, 13,  1,  8,  7, 22, 16, 11, 10,  1, 17, 20, 25, 10, 17, 28,  1,  7,\n        24,  1, 19, 15, 13, 14, 24,  2,  1,  6, 14, 11,  1, 22,  7, 15, 19,  1,\n        12,  7, 17, 17, 23,  1, 23, 20, 12, 24, 17, 28,  1, 20, 19,  1, 24, 14,\n        11,  1, 13, 22, 20, 25, 19, 10,  2,  1,  6, 14, 11,  1, 23, 24,  7, 22,\n        23,  1, 24, 27, 15, 19, 16, 17, 11,  1, 15, 19,  1, 24, 14, 11,  1,  9,\n        17, 11,  7, 22,  1, 19, 15, 13, 14, 24,  1, 23, 16, 28,  2,  1,  3,  1,\n        18,  7, 19,  1, 27,  7, 17, 16, 23,  1, 14, 15, 23,  1, 10, 20, 13,  1,\n        11, 26, 11, 22, 28,  1, 11, 26, 11, 19, 15, 19, 13,  2,  1,  4, 15, 22,\n        10, 23,  1, 23, 15, 19, 13,  1, 15, 19,  1, 24, 14, 11,  1, 18, 20, 22,\n        19, 15, 19, 13,  1, 17, 15, 13, 14, 24,  2,  1,  3,  1, 18,  7, 19,  1,\n        27,  7, 17, 16, 23,  1, 14, 15, 23,  1, 10, 20, 13,  1, 11, 26, 11, 22,\n        28,  1, 11, 26, 11, 19, 15, 19, 13,  2,  1,  6, 14, 11,  1, 22,  7, 15,\n        19,  1, 12,  7, 17, 17, 23,  1, 23, 20, 12, 24, 17, 28,  1, 20, 19,  1,\n        24, 14, 11,  1, 13, 22, 20, 25, 19, 10,  2,  1,  6, 14, 11,  1,  9,  7,\n        24,  1, 23,  7, 24,  1, 20, 19,  1, 24, 14, 11,  1, 18,  7, 24,  2,  1,\n         6, 14, 11,  1, 23, 25, 19,  1, 23, 14, 15, 19, 11, 23,  1,  8, 22, 15,\n        13, 14, 24,  1, 15, 19,  1, 24, 14, 11,  1, 23, 16, 28,  2,  1,  3,  1,\n        18,  7, 19,  1, 27,  7, 17, 16, 23,  1, 14, 15, 23,  1, 10, 20, 13,  1,\n        11, 26, 11, 22, 28,  1, 11, 26, 11, 19, 15, 19, 13,  2,  1,  6, 14, 11,\n         1,  9,  7, 24,  1, 23,  7, 24,  1, 20, 19,  1, 24, 14, 11,  1, 18,  7,\n        24,  2,  1,  6, 14, 11,  1,  9,  7, 24,  1, 23,  7, 24,  1, 20, 19,  1,\n        24, 14, 11,  1, 18,  7, 24,  2,  1,  6, 14, 11,  1, 23, 24,  7, 22, 23,\n         1, 24, 27, 15, 19, 16, 17, 11,  1, 15, 19,  1, 24, 14, 11,  1,  9, 17,\n        11,  7, 22,  1, 19, 15, 13, 14, 24,  1, 23, 16, 28,  2,  1,  3,  1, 18,\n         7, 19,  1, 27,  7, 17, 16, 23,  1, 14, 15, 23,  1, 10, 20, 13,  1, 11,\n        26, 11, 22, 28,  1, 11, 26, 11, 19, 15, 19, 13,  2,  1,  6, 14, 11,  1,\n        24, 22, 11, 11,  1, 23, 24,  7, 19, 10, 23,  1, 24,  7, 17, 17,  1,  7,\n        19, 10,  1, 23, 24, 22, 20, 19, 13,  2,  1,  6, 14, 11,  1, 23, 25, 19,\n         1, 23, 14, 15, 19, 11, 23,  1,  8, 22, 15, 13, 14, 24,  1, 15, 19,  1,\n        24, 14, 11,  1, 23, 16, 28,  2,  1,  6, 14, 11,  1, 23, 24,  7, 22, 23,\n         1, 24, 27, 15, 19, 16, 17, 11,  1, 15, 19,  1, 24, 14, 11,  1,  9, 17,\n        11,  7, 22,  1, 19, 15, 13, 14, 24,  1, 23, 16, 28,  2,  0,  0,  6, 14,\n        11,  1,  9,  7, 24,  1, 23,  7, 24,  1, 20, 19,  1, 24, 14, 11,  1, 18,\n         7, 24,  2,  1,  5, 14, 15, 17, 10, 22, 11, 19,  1, 21, 17,  7, 28,  1,\n        14,  7, 21, 21, 15, 17, 28,  1, 15, 19,  1, 24, 14, 11,  1, 21,  7, 22,\n        16,  2,  1,  3,  1, 13, 11, 19, 24, 17, 11,  1,  8, 22, 11, 11, 29, 11,\n         1, 18, 20, 26, 11, 23,  1, 24, 14, 11,  1, 17, 11,  7, 26, 11, 23,  2,\n         1,  3,  1, 13, 11, 19, 24, 17, 11,  1,  8, 22, 11, 11, 29, 11,  1, 18,\n        20, 26, 11, 23,  1, 24, 14, 11,  1, 17, 11,  7, 26, 11, 23,  2,  1,  4,\n        15, 22, 10, 23,  1, 23, 15, 19, 13,  1, 15, 19,  1, 24, 14, 11,  1, 18,\n        20, 22, 19, 15, 19, 13,  1, 17, 15, 13, 14, 24,  2,  1,  6, 14, 11,  1,\n        10, 20, 13,  1,  8,  7, 22, 16, 11, 10,  1, 17, 20, 25, 10, 17, 28,  1,\n         7, 24,  1, 19, 15, 13, 14, 24,  2,  1,  6, 14, 11,  1, 23, 24,  7, 22,\n        23,  1, 24, 27, 15, 19, 16, 17, 11,  1, 15, 19,  1, 24, 14, 11,  1,  9,\n        17, 11,  7, 22,  1, 19, 15, 13, 14, 24,  1, 23, 16, 28,  2,  1,  5, 14,\n        15, 17, 10, 22, 11, 19,  1, 21, 17,  7, 28,  1, 14,  7, 21, 21, 15, 17,\n        28,  1, 15, 19,  1, 24, 14, 11,  1, 21,  7, 22, 16,  2,  1,  6, 14, 11,\n         1, 24, 22, 11, 11,  1, 23, 24,  7, 19, 10, 23,  1, 24,  7, 17, 17,  1,\n         7, 19, 10,  1, 23, 24, 22, 20, 19, 13,  2,  1,  6, 14, 11,  1, 22,  7,\n        15, 19,  1, 12,  7, 17, 17, 23,  1, 23])\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:15.350845Z","iopub.execute_input":"2025-06-19T12:10:15.351164Z","iopub.status.idle":"2025-06-19T12:10:15.355937Z","shell.execute_reply.started":"2025-06-19T12:10:15.351140Z","shell.execute_reply":"2025-06-19T12:10:15.354977Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:19.196382Z","iopub.execute_input":"2025-06-19T12:10:19.196707Z","iopub.status.idle":"2025-06-19T12:10:19.203893Z","shell.execute_reply.started":"2025-06-19T12:10:19.196682Z","shell.execute_reply":"2025-06-19T12:10:19.202993Z"}},"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"tensor([ 6, 14, 11,  1, 23, 25, 19,  1, 23])"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:22.076132Z","iopub.execute_input":"2025-06-19T12:10:22.076443Z","iopub.status.idle":"2025-06-19T12:10:22.085679Z","shell.execute_reply.started":"2025-06-19T12:10:22.076419Z","shell.execute_reply":"2025-06-19T12:10:22.084694Z"}},"outputs":[{"name":"stdout","text":"when input is tensor([6]) the target: 14\nwhen input is tensor([ 6, 14]) the target: 11\nwhen input is tensor([ 6, 14, 11]) the target: 1\nwhen input is tensor([ 6, 14, 11,  1]) the target: 23\nwhen input is tensor([ 6, 14, 11,  1, 23]) the target: 25\nwhen input is tensor([ 6, 14, 11,  1, 23, 25]) the target: 19\nwhen input is tensor([ 6, 14, 11,  1, 23, 25, 19]) the target: 1\nwhen input is tensor([ 6, 14, 11,  1, 23, 25, 19,  1]) the target: 23\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:25.358083Z","iopub.execute_input":"2025-06-19T12:10:25.358372Z","iopub.status.idle":"2025-06-19T12:10:25.371355Z","shell.execute_reply.started":"2025-06-19T12:10:25.358349Z","shell.execute_reply":"2025-06-19T12:10:25.370385Z"}},"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[14,  7, 21, 21, 15, 17, 28,  1],\n        [ 1,  6, 14, 11,  1, 23, 25, 19],\n        [23,  2,  1,  6, 14, 11,  1, 24],\n        [ 0,  0,  6, 14, 11,  1, 23, 25]])\ntargets:\ntorch.Size([4, 8])\ntensor([[ 7, 21, 21, 15, 17, 28,  1, 15],\n        [ 6, 14, 11,  1, 23, 25, 19,  1],\n        [ 2,  1,  6, 14, 11,  1, 24, 22],\n        [ 0,  6, 14, 11,  1, 23, 25, 19]])\n----\nwhen input is [14] the target: 7\nwhen input is [14, 7] the target: 21\nwhen input is [14, 7, 21] the target: 21\nwhen input is [14, 7, 21, 21] the target: 15\nwhen input is [14, 7, 21, 21, 15] the target: 17\nwhen input is [14, 7, 21, 21, 15, 17] the target: 28\nwhen input is [14, 7, 21, 21, 15, 17, 28] the target: 1\nwhen input is [14, 7, 21, 21, 15, 17, 28, 1] the target: 15\nwhen input is [1] the target: 6\nwhen input is [1, 6] the target: 14\nwhen input is [1, 6, 14] the target: 11\nwhen input is [1, 6, 14, 11] the target: 1\nwhen input is [1, 6, 14, 11, 1] the target: 23\nwhen input is [1, 6, 14, 11, 1, 23] the target: 25\nwhen input is [1, 6, 14, 11, 1, 23, 25] the target: 19\nwhen input is [1, 6, 14, 11, 1, 23, 25, 19] the target: 1\nwhen input is [23] the target: 2\nwhen input is [23, 2] the target: 1\nwhen input is [23, 2, 1] the target: 6\nwhen input is [23, 2, 1, 6] the target: 14\nwhen input is [23, 2, 1, 6, 14] the target: 11\nwhen input is [23, 2, 1, 6, 14, 11] the target: 1\nwhen input is [23, 2, 1, 6, 14, 11, 1] the target: 24\nwhen input is [23, 2, 1, 6, 14, 11, 1, 24] the target: 22\nwhen input is [0] the target: 0\nwhen input is [0, 0] the target: 6\nwhen input is [0, 0, 6] the target: 14\nwhen input is [0, 0, 6, 14] the target: 11\nwhen input is [0, 0, 6, 14, 11] the target: 1\nwhen input is [0, 0, 6, 14, 11, 1] the target: 23\nwhen input is [0, 0, 6, 14, 11, 1, 23] the target: 25\nwhen input is [0, 0, 6, 14, 11, 1, 23, 25] the target: 19\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"print(xb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:31.034358Z","iopub.execute_input":"2025-06-19T12:10:31.034729Z","iopub.status.idle":"2025-06-19T12:10:31.040915Z","shell.execute_reply.started":"2025-06-19T12:10:31.034704Z","shell.execute_reply":"2025-06-19T12:10:31.040013Z"}},"outputs":[{"name":"stdout","text":"tensor([[14,  7, 21, 21, 15, 17, 28,  1],\n        [ 1,  6, 14, 11,  1, 23, 25, 19],\n        [23,  2,  1,  6, 14, 11,  1, 24],\n        [ 0,  0,  6, 14, 11,  1, 23, 25]])\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:33.894275Z","iopub.execute_input":"2025-06-19T12:10:33.894574Z","iopub.status.idle":"2025-06-19T12:10:33.924047Z","shell.execute_reply.started":"2025-06-19T12:10:33.894551Z","shell.execute_reply":"2025-06-19T12:10:33.922676Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 30])\ntensor(3.4631, grad_fn=<NllLossBackward0>)\n\nndoozbbnoh ywapesppb\nhlcnuccrCnlTvBtbonczpyivwobsw\nkcn tB.CgmmaetaCAlgTBsplAzmczh tCdlAnwtadgTBwuphv\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:38.485797Z","iopub.execute_input":"2025-06-19T12:10:38.486405Z","iopub.status.idle":"2025-06-19T12:10:38.491524Z","shell.execute_reply.started":"2025-06-19T12:10:38.486376Z","shell.execute_reply":"2025-06-19T12:10:38.490453Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"batch_size = 32\nfor steps in range(5000): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:40.941637Z","iopub.execute_input":"2025-06-19T12:10:40.941934Z","iopub.status.idle":"2025-06-19T12:10:50.371752Z","shell.execute_reply.started":"2025-06-19T12:10:40.941914Z","shell.execute_reply":"2025-06-19T12:10:50.370919Z"}},"outputs":[{"name":"stdout","text":"1.7273764610290527\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:10:54.387023Z","iopub.execute_input":"2025-06-19T12:10:54.387315Z","iopub.status.idle":"2025-06-19T12:10:54.713395Z","shell.execute_reply.started":"2025-06-19T12:10:54.387294Z","shell.execute_reply":"2025-06-19T12:10:54.712571Z"}},"outputs":[{"name":"stdout","text":"\n\neve sary farmain als A Thestarrs sththealrs  cared g. tapath\n\n\n\nThe d mapigheshe mA moung. ththe t thezerCChe doun g at. h panghire tht oghezeze Thesir t thevestlat. t. ildoge the heze onghtmoghe sontlontheen t. The The on n pppinghe mogrees. thtrksuds s s tA makle t tathtly ve wat. atrarCghe catatheally. tly cay sunain cap. ron tathat. t ont. A the The s g t s Bfzee ld\nThe mnght wfan hthematrt tappinhis matay.  s nky.\nA e n g s ppzet len re y gThe sales ppTht t. llks t sigks wig. le the heacl mat htunt  the he Ththin hards d. cay pinge t. nilary tCBndofaCgrkaneve orker she  leThin het hed cal the cle sarnt. twightaveerkt. nd t dron taroftrdoA stbaininin seg in may th thin hees wandsat pilentmoft brk. arerks he A pin marn singhtun t t. st he ly. tappgrBighingrCpves tles ban Theshe in pin the s. in rdshentwalA caintat. ly Cfohet.hee is ly s t g. pllig t sthe aly. lesthare e dlmatrog t cle drigepin Bin Cze tlesat g. brang sun Thes ly A nczersthe Bintalesun bply thishininds ccay theds oraligh CBin st The tatathind Tht s nthe gwy. A ppinky.  n t. cve t ivendstung oun trerklet  d lly. s A nun gre bree bat. fatrng brklly ohebrove ban Ththay atht ry. ghtatat moy nifary. Tht. m plsuBig. the ig sth nghenBistat lkl fave ig. Theze taldounr brky thee linir cunighardoundsthe s Thee thecleske ain The le\nA sthigheveze lsorBisthBimave mare ls Big he s A rindsky og tAks ofavevBdcly t. span maing sonig. r vds. sksky Thighun t arChe Barogroudstan ry mog. t hle A he A st waCht. Thtr n Thezezesh g r s tldroudlvesile  bry grCis. tlay ig Cg. in A s he pand. inin wig ly t n sht \n\nBin Bftrs t eands og.\nkldr llogbrarat Thesppin endd avezendog hen he in saran pppilohunirkle skhee Theve s muntavpllsonge ore on n thin ly TheTheds trnthaly. s thetoundsuds tbrene ghindoun st ds t caloun twand Chen doniroghee pppariinmon sighenlky ward taledoft. ftllly. rdstrCg s capany an st. inBivhtw\nppan okzeresthen dlig. nd ingr irc. thesta. st Theske ig mofaB. gherksthe\n\ne c ev\n\nThth sun inrds igrn n A Thin \n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:03.934182Z","iopub.execute_input":"2025-06-19T12:11:03.934453Z","iopub.status.idle":"2025-06-19T12:11:03.946058Z","shell.execute_reply.started":"2025-06-19T12:11:03.934435Z","shell.execute_reply":"2025-06-19T12:11:03.945075Z"}},"outputs":[{"name":"stdout","text":"a=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"# consider the following toy example:\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:06.805022Z","iopub.execute_input":"2025-06-19T12:11:06.805301Z","iopub.status.idle":"2025-06-19T12:11:06.813778Z","shell.execute_reply.started":"2025-06-19T12:11:06.805282Z","shell.execute_reply":"2025-06-19T12:11:06.812795Z"}},"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":122},{"cell_type":"code","source":"# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:09.176952Z","iopub.execute_input":"2025-06-19T12:11:09.177251Z","iopub.status.idle":"2025-06-19T12:11:09.185342Z","shell.execute_reply.started":"2025-06-19T12:11:09.177227Z","shell.execute_reply":"2025-06-19T12:11:09.184431Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:12.422395Z","iopub.execute_input":"2025-06-19T12:11:12.422781Z","iopub.status.idle":"2025-06-19T12:11:12.431508Z","shell.execute_reply.started":"2025-06-19T12:11:12.422755Z","shell.execute_reply":"2025-06-19T12:11:12.430127Z"}},"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:17.286895Z","iopub.execute_input":"2025-06-19T12:11:17.287162Z","iopub.status.idle":"2025-06-19T12:11:17.295172Z","shell.execute_reply.started":"2025-06-19T12:11:17.287145Z","shell.execute_reply":"2025-06-19T12:11:17.294068Z"}},"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":126},{"cell_type":"code","source":"# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:19.294624Z","iopub.execute_input":"2025-06-19T12:11:19.295387Z","iopub.status.idle":"2025-06-19T12:11:19.311321Z","shell.execute_reply.started":"2025-06-19T12:11:19.295356Z","shell.execute_reply":"2025-06-19T12:11:19.310359Z"}},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"wei[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:23.445574Z","iopub.execute_input":"2025-06-19T12:11:23.445958Z","iopub.status.idle":"2025-06-19T12:11:23.453737Z","shell.execute_reply.started":"2025-06-19T12:11:23.445935Z","shell.execute_reply":"2025-06-19T12:11:23.452792Z"}},"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":128},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:11:26.043961Z","iopub.execute_input":"2025-06-19T12:11:26.044910Z","iopub.status.idle":"2025-06-19T12:11:26.050956Z","shell.execute_reply.started":"2025-06-19T12:11:26.044880Z","shell.execute_reply":"2025-06-19T12:11:26.049846Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"k.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:55:19.360449Z","iopub.execute_input":"2025-06-19T12:55:19.360784Z","iopub.status.idle":"2025-06-19T12:55:19.369449Z","shell.execute_reply.started":"2025-06-19T12:55:19.360756Z","shell.execute_reply":"2025-06-19T12:55:19.368656Z"}},"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"tensor(1.0449)"},"metadata":{}}],"execution_count":174},{"cell_type":"code","source":"q.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:55:29.495134Z","iopub.execute_input":"2025-06-19T12:55:29.495405Z","iopub.status.idle":"2025-06-19T12:55:29.502318Z","shell.execute_reply.started":"2025-06-19T12:55:29.495386Z","shell.execute_reply":"2025-06-19T12:55:29.501395Z"}},"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"tensor(1.0700)"},"metadata":{}}],"execution_count":175},{"cell_type":"code","source":"wei.var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:55:42.676477Z","iopub.execute_input":"2025-06-19T12:55:42.676853Z","iopub.status.idle":"2025-06-19T12:55:42.685665Z","shell.execute_reply.started":"2025-06-19T12:55:42.676828Z","shell.execute_reply":"2025-06-19T12:55:42.684786Z"}},"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"tensor(1.0918)"},"metadata":{}}],"execution_count":176},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:55:57.630324Z","iopub.execute_input":"2025-06-19T12:55:57.630656Z","iopub.status.idle":"2025-06-19T12:55:57.638238Z","shell.execute_reply.started":"2025-06-19T12:55:57.630631Z","shell.execute_reply":"2025-06-19T12:55:57.637435Z"}},"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"},"metadata":{}}],"execution_count":177},{"cell_type":"code","source":"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:56:35.106160Z","iopub.execute_input":"2025-06-19T12:56:35.106429Z","iopub.status.idle":"2025-06-19T12:56:35.113754Z","shell.execute_reply.started":"2025-06-19T12:56:35.106410Z","shell.execute_reply":"2025-06-19T12:56:35.112731Z"}},"outputs":[{"execution_count":179,"output_type":"execute_result","data":{"text/plain":"tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"},"metadata":{}}],"execution_count":179},{"cell_type":"code","source":"class LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:56:37.787506Z","iopub.execute_input":"2025-06-19T12:56:37.787931Z","iopub.status.idle":"2025-06-19T12:56:37.803840Z","shell.execute_reply.started":"2025-06-19T12:56:37.787902Z","shell.execute_reply":"2025-06-19T12:56:37.803025Z"}},"outputs":[{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 100])"},"metadata":{}}],"execution_count":180},{"cell_type":"code","source":"x[:,0].mean(), x[:,0].std()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:56:41.105351Z","iopub.execute_input":"2025-06-19T12:56:41.106134Z","iopub.status.idle":"2025-06-19T12:56:41.113779Z","shell.execute_reply.started":"2025-06-19T12:56:41.106105Z","shell.execute_reply":"2025-06-19T12:56:41.113003Z"}},"outputs":[{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"(tensor(0.1469), tensor(0.8803))"},"metadata":{}}],"execution_count":181},{"cell_type":"code","source":"x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:56:43.855973Z","iopub.execute_input":"2025-06-19T12:56:43.856510Z","iopub.status.idle":"2025-06-19T12:56:43.864080Z","shell.execute_reply.started":"2025-06-19T12:56:43.856488Z","shell.execute_reply":"2025-06-19T12:56:43.863320Z"}},"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"(tensor(-9.5367e-09), tensor(1.0000))"},"metadata":{}}],"execution_count":182},{"cell_type":"code","source":"print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=2000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:56:46.786323Z","iopub.execute_input":"2025-06-19T12:56:46.786662Z","iopub.status.idle":"2025-06-19T12:56:47.099059Z","shell.execute_reply.started":"2025-06-19T12:56:46.786633Z","shell.execute_reply":"2025-06-19T12:56:47.098055Z"}},"outputs":[{"name":"stdout","text":"\nA mat maly trong. A s sog s fthe ararkeaCg gheve sthe twatllly. malkA dngheat talounghtwindsTh s. sondwary Thtly. carky. rondsthedoveveat she tbro\n\nin hemoudly aveng Bing. ge The s Thtvenghevecn Thtlles atared. s ds manonk. bhe ing ig Thee are tandspanin ealogpat. heee The Thtbrken lky brntly.\nThlly ghthly t thbroudofa\n\nbrs ed. ily dog eninbrig t inky far on rezeanin montwin The \nTheestry. mat sun at. in \nrn htre Thtrindrnen ll mareat ThTheres stht brke she the tloud.oy n ay on in lernindsaleve br starain stherdrk. leve e s A stwinghmarCon hed. ightry ppig talle n ftat s leale Checapang. s at. stbappls loplonin ht. Cht on ing. appathalel d are t trkl A\nThe sthe tws moun s hetre in broun s  The matheeeve s Thinun ovTheve t g.alle ghe leatly thinghtht. s le A The l The ave taalind. s thong fingh Bun sCstre ly t alsthtunir\nBle The thanghtle g t s sun thee hundlly theds Athe d. apig.\narChe pl sound s san Then pin sar ig. A t s tverofkg Theshes. onds dlethtre Thy ig manBudsheat an s \nCBin inky drs hend. t. esumong A then t tarn Che fayay sudoventky. tanChpin s t. s thalThe tarkly Cbrksk. rat. tanileze e brinmoght s balyk. g brCcavea. s pze thtwis Bing.\n\n\nrs brendoght dreveengheearedounig s. d s es s Thy BBd tatrk. sAthed. stlks Ththedskskllove suning bay. tr Abrky. sudrst. trd. s Cn ninguniCheindrbar he male plles \nft trstbrongre Bin st lery ongheave tl t Alkly. pl thtronin s es indlly. illkoves Thang ndsat e he Thevly. CBthedsonmovevskybrilon s mn s t The t hesunky.udon Bin eveevee t. the st ig trinisileinindsky. wig. s sA g moft mon tbrningry st cly sat. sun en Thed oghe iniovenghears. bree eve pl lethisk. t re canightheves sing eedound ghe the moven ing. ly. irn n athe trkle thin ine sht The s Theen sthe gherestarm drds ing. apply in twardrengfthe\nThe ng. s tlareveve henalsarn Bivevhe bringeengmoud sthe evetrofatfat. Thee t. tangran gherk. maththe Cg. henthirCathes wightrs The inirk. brale ifalinghenghe the tinin brCgheatr sininin taren ig taily t. ft ing ig. marevee \n","output_type":"stream"}],"execution_count":183},{"cell_type":"code","source":"import torch\n\n\ncontext = \"com\"\n\n\ncontext_ids = torch.tensor([encode(context)], dtype=torch.long)\nprint(f\"Encoded context: {context_ids}\")\n\n\noutput = m.generate(idx=context_ids, max_new_tokens=1)\n\nnext_char_id = output[0][-1].item()\nnext_char = decode([next_char_id])\n\nprint(f\"Given '{context}' => Next character predicted: '{next_char}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T12:57:04.084631Z","iopub.execute_input":"2025-06-19T12:57:04.084942Z","iopub.status.idle":"2025-06-19T12:57:04.092132Z","shell.execute_reply.started":"2025-06-19T12:57:04.084921Z","shell.execute_reply":"2025-06-19T12:57:04.091363Z"}},"outputs":[{"name":"stdout","text":"Encoded context: tensor([[ 9, 20, 18]])\nGiven 'com' => Next character predicted: 'o'\n","output_type":"stream"}],"execution_count":185}]}