# Transformer_from_scratch
This project implements a simple character-level language model in PyTorch to demonstrate the difference between a basic Bigram model and an Attention-based model. 
The Bigram model predicts the next character based only on the current one, which results in low accuracy and limited text coherence. To improve this, I replaced it with an Attention mechanism that allows the model to learn longer-range dependencies between characters. With self-attention, the model achieved significantly higher accuracy and generates much more meaningful text, highlighting how powerful attention is for sequence modeling tasks
